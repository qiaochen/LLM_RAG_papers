### Representation editing

-  Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., & Potts, C. (2024). [ReFT: Representation Finetuning for Language Models](https://arxiv.org/abs/2404.03592). _arXiv preprint arXiv:2404.03592_.


### [Knowledge editing](https://github.com/zjunlp/KnowledgeEditingPapers)



### Embedding
[MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)


- [LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/abs/2404.05961)
> Very nice paper taught me how to transform a decoder-only LLM to bidirectional encoder. The gap between a decoder-only LLM and a good encoder is the attention mechanism: decoder-only means causal attention (a token's representation is generated by attending its prior tokens); for a encoder, both prior and future context should be exploited for a better representation.
> This work's strategy: 1) convert causal attention mask to non-causal; 2) fine tune with bidirectional next token prediction (a token be predicted using its previous and future token context, separately in loss computation, but together; they use next-to current token's representation to align with pretraining objective); 3) sentence level contrastive learning for overall text representation.
> Apart from top performance, their model analysis also show interesting model behaviors. They try to investigate the impact of bidirectional attention, with a dataset where sentences share same prefix but may have different meaning with later part, this can allow the bidirectional encoder to excel in token level tasks at the prefix part, since they can gain insights from future tokens.  Moreover, particularly for Mistral 7B chat, they find the token wise embeddings between uni and bi attention are much more consistent than other models, which may indicate there can be some bidirectional attention training strategies in Mistral 7B training.
> Overall, this is a clever and computationally more efficient

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTgzMjM5Mjc0MSwtMTg5ODQ0ODY1MywtNj
k4OTg3NDc3LC0yMDEyNjAzMjAxLC00NTc1NDY0MDIsLTEwMDg0
MDMwNDFdfQ==
-->