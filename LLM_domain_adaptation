### Representation editing

-  Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., & Potts, C. (2024). [ReFT: Representation Finetuning for Language Models](https://arxiv.org/abs/2404.03592). _arXiv preprint arXiv:2404.03592_.


### [Knowledge editing](https://github.com/zjunlp/KnowledgeEditingPapers)

### Fine-tuning
Kim, H., Hwang, H., Lee, J., Park, S., Kim, D., Lee, T., ... & Kang, J. (2024). [Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks](https://arxiv.org/abs/2404.00376v1). _arXiv preprint arXiv:2404.00376_.

### Embedding
[MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)


- [LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/abs/2404.05961)
> Very nice paper taught me how to transform a decoder-only LLM to bidirectional encoder. The gap between a decoder-only LLM and a good encoder is the attention mechanism: decoder-only means causal attention (a token's representation is generated by attending its prior tokens); for a encoder, both prior and future context should be exploited for a better representation.
> This work's strategy: 1) convert causal attention mask to non-causal; 2) fine tune with bidirectional next token prediction (a token be predicted using its previous and future token context, separately in loss computation, but together; they use next-to current token's representation to align with pretraining objective); 3) sentence level contrastive learning for overall text representation.
> Apart from top performance, their model analysis also show interesting model behaviors. They try to investigate the impact of bidirectional attention, with a dataset where sentences share same prefix but may have different meaning with later part, this can allow the bidirectional encoder to excel in token level tasks at the prefix part, since they can gain insights from future tokens.  Moreover, particularly for Mistral 7B chat, they find the token wise embeddings between uni and bi attention are much more consistent than other models, which may indicate there can be some bidirectional attention training strategies in Mistral 7B training.
> Overall, this is a clever and computationally more efficient way of turning decoder-llm's into universal text encoders.

<!--stackedit_data:
eyJoaXN0b3J5IjpbMjA4NTQ2OTg1MiwtNTE2OTY0NDkxLC0xOD
k4NDQ4NjUzLC02OTg5ODc0NzcsLTIwMTI2MDMyMDEsLTQ1NzU0
NjQwMiwtMTAwODQwMzA0MV19
-->