### Representation editing

-  Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., & Potts, C. (2024). [ReFT: Representation Finetuning for Language Models](https://arxiv.org/abs/2404.03592). _arXiv preprint arXiv:2404.03592_.


### [Knowledge editing](https://github.com/zjunlp/KnowledgeEditingPapers)



### Embedding
[MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)


- [LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/abs/2404.05961)
> Very nice paper taught me how to transform a decoder-only LLM to bidirectional encoder. The gap between a decoder-only LLM and a good encoder is the attention mechanism: decoder-only means causal attention (a token's representation is generated by attending its prior tokens); for a encoder, both prior and future context should be exploited for a better representation.
> This work's strategy: 1) convert causal attention mask to non-causal; 2) fine tune using  

<!--stackedit_data:
eyJoaXN0b3J5IjpbOTIyNzY2MjgxLC02OTg5ODc0NzcsLTIwMT
I2MDMyMDEsLTQ1NzU0NjQwMiwtMTAwODQwMzA0MV19
-->