#### Keywords:
- Large Language Models (LLM): 
	- [Transformer](https://arxiv.org/abs/1706.03762) architecture-based:
		- Decoder-only Transformers, e.g. [GPT](https://paperswithcode.com/paper/improving-language-understanding-by) and its extensions
		- Encoder-only Transformers, e.g., [BERT](https://arxiv.org/abs/1810.04805) and its extensions
		- Complete Transformers with both encoder and decoders, e.g., [BART](https://arxiv.org/abs/1910.13461), [T-5](https://arxiv.org/abs/1910.10683)
	- [Mamba](https://arxiv.org/abs/2312.00752)
		
- Retrieval-augmented generation (RAG):
	- 
- Fine tuning
- Instruct fine tuning


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1MjM5NzQxNzQsMTg3ODAxNTc1NiwtMj
A4ODc0NjYxMl19
-->